import warnings
warnings.filterwarnings('ignore')
import os, sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime
import seaborn as sns
import scipy
import random
import math
from sklearn.ensemble import RandomForestClassifier
from sklearn import preprocessing
from sklearn import metrics

#calling out the file from local
data = pd.read_csv('marketing_campaign.csv')

#data pre-processing
data['Income'].fillna(np.mean(data['Income']), inplace = True)

data.drop(['Z_CostContact','Z_Revenue'], axis = 1, inplace=True)
data.drop(['ID','Dt_Customer'], axis=1, inplace=True)

data['Age'] = 2021 - data.Year_Birth.to_numpy()
data.drop('Year_Birth', axis=1, inplace=True)

data['Education'] = data['Education'].replace(['Basic'],'Undergraduate')
data['Education'] = data['Education'].replace(['Graduation'],'Graduate')
data['Education'] = data['Education'].replace(['Master','2n Cycle','PhD'],'Postgraduate')

data['Income'] = np.where(data['Income'] > 120000, 120000, data['Income'])
data['Age'] = np.where(data['Age'] > 90, 90, data['Age'])

df_cluster = data.copy()

data['Education'] = data['Education'].replace(['Basic'], 0)
data['Education'] = data['Education'].replace(['Bachelor'],1)
data['Education'] = data['Education'].replace(['Master'],2)
data['Education'] = data['Education'].replace(['PhD'],3)

data = pd.get_dummies(data)





clf = RandomForestClassifier()
clf.fit(data.drop('Response', axis=1), data['Response'])
plt.style.use('seaborn-whitegrid')
importance = clf.feature_importances_
importance = pd.DataFrame(importance, index=data.drop('Response', axis=1).columns, columns=["Importance"])
importance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)/2));
plt.show()
# Choose Features (Keep 90% Importance ration)
feature_nums = 18
ascend_import = importance.sort_values(by='Importance', ascending=False)
all_info = ascend_import['Importance'].iloc[:feature_nums].sum()
all_choose_features = list(ascend_import.iloc[:feature_nums].index)
print('Names: ', all_choose_features)
print('Importance Raio: ', all_info)


df_cluster['Marital_Status'] = df_cluster['Marital_Status'].replace(['Widow','Divorced'],'Single')
df_cluster['Marital_Status'] = df_cluster['Marital_Status'].replace(['Married','Together'],'Relationship')

df_cluster['Children'] = df_cluster['Kidhome'] + df_cluster['Teenhome']
df_cluster['MntTotal'] = np.sum(df_cluster.filter(regex = 'Mnt'), axis = 1)
df_cluster['NumTotal'] = np.sum(df_cluster.filter(regex = 'Purchases'), axis = 1)
df_cluster['TotalAccepted'] = np.sum(df_cluster.filter(regex='Accepted'), axis=1)
df_cluster.drop(columns=['AcceptedCmp3','AcceptedCmp4','AcceptedCmp5','AcceptedCmp1','AcceptedCmp2'],inplace=True)
df_cluster['AvgWeb'] = round(df_cluster['NumWebPurchases'] / df_cluster['NumWebVisitsMonth'], 2)
df_cluster.fillna({'AvgWeb' : 0}, inplace=True)
df_cluster.replace(np.inf,0,inplace=True)

import sklearn.cluster as cluster
from sklearn.preprocessing import LabelEncoder
encode = LabelEncoder()
for i in['Education','Marital_Status']:
    df_cluster[i] = df_cluster[[i]].apply(encode.fit_transform)

cluster_data = df_cluster.values

std_scale = preprocessing.StandardScaler().fit(cluster_data)
cluster_data = std_scale.transform(cluster_data)

K=range(1,14)
wss = []
for k in K:
    kmeans = cluster.KMeans(n_clusters=k, init="k-means++")
    kmeans=kmeans.fit(cluster_data)
    wss_iter = kmeans.inertia_
    wss.append(wss_iter)

mycenters = pd.DataFrame({'Clusters' : K, 'WSS' : wss})
print(mycenters)

sns.scatterplot(x = 'Clusters', y = 'WSS', data = mycenters, marker="+")
plt.show()

import sklearn.metrics as metrics

for i in range(2,13):
    labels = cluster.KMeans(n_clusters=i, init="k-means++", random_state=200).fit(cluster_data).labels_
    print("Silhouette score for k(clusters) = "+str(i)+" is "
          +str(metrics.silhouette_score(cluster_data,labels,metric="euclidean",sample_size=1000,random_state = 200)))

from yellowbrick.cluster import KElbowVisualizer
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=2, init = 'k-means++').fit(cluster_data)
pred = kmeans.predict(cluster_data)
df_cluster['Cluster'] = pred + 1

print(df_cluster["Cluster"].value_counts())

sns.scatterplot(x='Income', y='MntTotal', hue='Cluster', data=df_cluster)
plt.show()


visualizer = KElbowVisualizer(kmeans, k=(2,15), metric='silhouette')
visualizer.fit(cluster_data)
visualizer.finalize()
plt.show()

visualizer = KElbowVisualizer(kmeans, k=(2,15), metric = 'calinski_harabasz')
visualizer.fit(cluster_data)
visualizer.finalize()
plt.show()
for i in df_cluster:
    g = sns.FacetGrid(df_cluster, col = "Cluster", hue = "Cluster", palette = "coolwarm", sharey = False, sharex = False)
    g.map(sns.histplot,i)

    g.set_xticklabels(rotation=30)
    g.set_yticklabels()
    g.fig.set_figheight(5)
    g.fig.set_figwidth(20)
plt.show()

from sklearn.datasets import make_blobs
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering

dendrogram = sch.dendrogram(sch.linkage(cluster_data, method = 'ward'))
plt.show()

hc = AgglomerativeClustering(n_clusters=2, affinity = 'euclidean', linkage = 'ward')
y_hc = hc.fit_predict(cluster_data)

visualizer = KElbowVisualizer(hc, k=(2,15), metric='silhouette')
visualizer.fit(cluster_data)
visualizer.finalize()
plt.show()

visualizer = KElbowVisualizer(hc, k=(2,15), metric = 'calinski_harabasz')
visualizer.fit(cluster_data)
visualizer.finalize()
plt.show()
